{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1d7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core Python / System ===\n",
    "import os\n",
    "\n",
    "# === Data Handling ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Scikit-learn: Preprocessing, Splitting, Evaluation ===\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# === PyTorch: Core, Models, Optim, Utils ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775272da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 180596\n",
      "Feature dimension: 78\n"
     ]
    }
   ],
   "source": [
    "# === PyTorch: Set device ===\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Drop columns with all NaNs or unnamed indices\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Replace inf/-inf with NaN, then fill NaNs with 0\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Encode labels: 1 for attack, 0 for normal\n",
    "df['Label'] = df['Label'].apply(lambda x: 0 if 'BENIGN' in x else 1)\n",
    "\n",
    "# Drop non-numeric/categorical columns if any\n",
    "non_numerics = df.select_dtypes(include=['object']).columns\n",
    "df = df.drop(non_numerics.difference(['Label']), axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('Label', axis=1).values\n",
    "y = df['Label'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Feature dimension:\", X_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b392a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack sample count: 102283\n",
      "Feature dimension: 78\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch datasets\n",
    "# Extract attack samples (label = 1)\n",
    "X_attack = X_train[y_train == 1]\n",
    "\n",
    "print(f\"Attack sample count: {X_attack.shape[0]}\")\n",
    "print(f\"Feature dimension: {X_attack.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd762ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | D Loss: 1.2838 | G Loss: 0.9839\n",
      "Epoch [2/100] | D Loss: 1.0026 | G Loss: 0.9653\n",
      "Epoch [3/100] | D Loss: 1.0443 | G Loss: 0.9967\n",
      "Epoch [4/100] | D Loss: 1.0955 | G Loss: 1.3717\n",
      "Epoch [5/100] | D Loss: 1.2105 | G Loss: 0.8957\n",
      "Epoch [6/100] | D Loss: 1.0916 | G Loss: 1.1129\n",
      "Epoch [7/100] | D Loss: 1.1721 | G Loss: 1.1737\n",
      "Epoch [8/100] | D Loss: 1.2574 | G Loss: 1.2661\n",
      "Epoch [9/100] | D Loss: 1.0849 | G Loss: 1.2260\n",
      "Epoch [10/100] | D Loss: 1.1224 | G Loss: 1.2387\n",
      "Epoch [11/100] | D Loss: 0.8911 | G Loss: 1.4563\n",
      "Epoch [12/100] | D Loss: 0.8432 | G Loss: 1.9138\n",
      "Epoch [13/100] | D Loss: 0.9891 | G Loss: 1.1659\n",
      "Epoch [14/100] | D Loss: 1.0835 | G Loss: 1.1176\n",
      "Epoch [15/100] | D Loss: 0.8273 | G Loss: 1.3246\n",
      "Epoch [16/100] | D Loss: 0.8162 | G Loss: 1.6889\n",
      "Epoch [17/100] | D Loss: 0.9587 | G Loss: 2.5905\n",
      "Epoch [18/100] | D Loss: 0.8299 | G Loss: 0.9611\n",
      "Epoch [19/100] | D Loss: 0.8508 | G Loss: 1.4989\n",
      "Epoch [20/100] | D Loss: 1.1027 | G Loss: 1.4324\n",
      "Epoch [21/100] | D Loss: 1.0718 | G Loss: 1.3680\n",
      "Epoch [22/100] | D Loss: 1.2051 | G Loss: 2.8524\n",
      "Epoch [23/100] | D Loss: 0.9695 | G Loss: 1.3911\n",
      "Epoch [24/100] | D Loss: 0.9729 | G Loss: 1.6684\n",
      "Epoch [25/100] | D Loss: 0.9414 | G Loss: 2.2370\n",
      "Epoch [26/100] | D Loss: 0.7907 | G Loss: 1.2761\n",
      "Epoch [27/100] | D Loss: 0.9068 | G Loss: 1.0209\n",
      "Epoch [28/100] | D Loss: 1.1869 | G Loss: 2.3704\n",
      "Epoch [29/100] | D Loss: 0.8092 | G Loss: 1.4280\n",
      "Epoch [30/100] | D Loss: 0.8163 | G Loss: 1.6872\n",
      "Epoch [31/100] | D Loss: 1.0552 | G Loss: 1.7996\n",
      "Epoch [32/100] | D Loss: 0.9288 | G Loss: 1.1269\n",
      "Epoch [33/100] | D Loss: 1.0578 | G Loss: 2.8218\n",
      "Epoch [34/100] | D Loss: 0.9734 | G Loss: 1.7519\n",
      "Epoch [35/100] | D Loss: 1.0588 | G Loss: 2.0532\n",
      "Epoch [36/100] | D Loss: 1.0261 | G Loss: 1.4913\n",
      "Epoch [37/100] | D Loss: 0.8849 | G Loss: 2.2548\n",
      "Epoch [38/100] | D Loss: 0.9577 | G Loss: 1.7881\n",
      "Epoch [39/100] | D Loss: 0.8257 | G Loss: 1.9044\n",
      "Epoch [40/100] | D Loss: 0.8985 | G Loss: 1.5009\n",
      "Epoch [41/100] | D Loss: 1.1273 | G Loss: 1.8959\n",
      "Epoch [42/100] | D Loss: 0.9033 | G Loss: 1.8175\n",
      "Epoch [43/100] | D Loss: 0.8308 | G Loss: 2.1383\n",
      "Epoch [44/100] | D Loss: 1.0129 | G Loss: 1.4818\n",
      "Epoch [45/100] | D Loss: 0.8939 | G Loss: 1.8206\n",
      "Epoch [46/100] | D Loss: 1.0338 | G Loss: 2.0033\n",
      "Epoch [47/100] | D Loss: 0.9031 | G Loss: 1.6160\n",
      "Epoch [48/100] | D Loss: 1.1417 | G Loss: 2.4463\n",
      "Epoch [49/100] | D Loss: 0.8722 | G Loss: 2.6051\n",
      "Epoch [50/100] | D Loss: 0.8798 | G Loss: 1.1756\n",
      "Epoch [51/100] | D Loss: 1.2061 | G Loss: 1.3552\n",
      "Epoch [52/100] | D Loss: 0.8734 | G Loss: 1.9525\n",
      "Epoch [53/100] | D Loss: 0.8199 | G Loss: 1.8653\n",
      "Epoch [54/100] | D Loss: 1.0296 | G Loss: 1.7610\n",
      "Epoch [55/100] | D Loss: 1.2293 | G Loss: 2.4992\n",
      "Epoch [56/100] | D Loss: 1.2963 | G Loss: 2.2585\n",
      "Epoch [57/100] | D Loss: 0.9669 | G Loss: 1.3546\n",
      "Epoch [58/100] | D Loss: 1.0986 | G Loss: 1.3268\n",
      "Epoch [59/100] | D Loss: 1.0404 | G Loss: 1.9509\n",
      "Epoch [60/100] | D Loss: 1.1832 | G Loss: 1.1306\n",
      "Epoch [61/100] | D Loss: 0.9927 | G Loss: 1.3677\n",
      "Epoch [62/100] | D Loss: 0.7689 | G Loss: 1.5192\n",
      "Epoch [63/100] | D Loss: 1.1751 | G Loss: 1.6150\n",
      "Epoch [64/100] | D Loss: 1.0927 | G Loss: 1.8553\n",
      "Epoch [65/100] | D Loss: 1.0698 | G Loss: 1.5201\n",
      "Epoch [66/100] | D Loss: 1.2624 | G Loss: 1.5581\n",
      "Epoch [67/100] | D Loss: 0.8565 | G Loss: 2.0024\n",
      "Epoch [68/100] | D Loss: 1.0451 | G Loss: 1.8452\n",
      "Epoch [69/100] | D Loss: 0.9774 | G Loss: 2.0244\n",
      "Epoch [70/100] | D Loss: 0.9248 | G Loss: 2.2033\n",
      "Epoch [71/100] | D Loss: 0.8947 | G Loss: 1.1895\n",
      "Epoch [72/100] | D Loss: 0.8524 | G Loss: 2.4707\n",
      "Epoch [73/100] | D Loss: 1.2219 | G Loss: 1.2050\n",
      "Epoch [74/100] | D Loss: 0.7563 | G Loss: 1.4922\n",
      "Epoch [75/100] | D Loss: 1.1633 | G Loss: 1.3779\n",
      "Epoch [76/100] | D Loss: 1.0057 | G Loss: 1.2512\n",
      "Epoch [77/100] | D Loss: 0.8885 | G Loss: 1.9259\n",
      "Epoch [78/100] | D Loss: 1.2555 | G Loss: 1.1656\n",
      "Epoch [79/100] | D Loss: 0.9971 | G Loss: 1.9162\n",
      "Epoch [80/100] | D Loss: 0.9881 | G Loss: 2.0938\n",
      "Epoch [81/100] | D Loss: 0.9651 | G Loss: 1.8012\n",
      "Epoch [82/100] | D Loss: 1.1501 | G Loss: 1.3796\n",
      "Epoch [83/100] | D Loss: 1.4214 | G Loss: 1.4501\n",
      "Epoch [84/100] | D Loss: 1.2283 | G Loss: 1.6111\n",
      "Epoch [85/100] | D Loss: 1.1458 | G Loss: 1.8769\n",
      "Epoch [86/100] | D Loss: 0.9435 | G Loss: 0.8931\n",
      "Epoch [87/100] | D Loss: 0.9084 | G Loss: 1.7917\n",
      "Epoch [88/100] | D Loss: 1.2394 | G Loss: 2.0321\n",
      "Epoch [89/100] | D Loss: 0.9025 | G Loss: 1.6863\n",
      "Epoch [90/100] | D Loss: 0.9311 | G Loss: 1.8695\n",
      "Epoch [91/100] | D Loss: 0.8849 | G Loss: 1.8620\n",
      "Epoch [92/100] | D Loss: 0.9228 | G Loss: 1.8622\n",
      "Epoch [93/100] | D Loss: 1.0167 | G Loss: 1.6288\n",
      "Epoch [94/100] | D Loss: 1.1332 | G Loss: 1.6519\n",
      "Epoch [95/100] | D Loss: 1.1534 | G Loss: 1.4818\n",
      "Epoch [96/100] | D Loss: 1.0983 | G Loss: 1.2661\n",
      "Epoch [97/100] | D Loss: 1.1031 | G Loss: 1.2454\n",
      "Epoch [98/100] | D Loss: 1.0392 | G Loss: 1.8172\n",
      "Epoch [99/100] | D Loss: 0.6553 | G Loss: 2.4064\n",
      "Epoch [100/100] | D Loss: 0.8112 | G Loss: 1.7486\n",
      " Synthetic attack data saved as 'synthetic_attacks.csv'\n"
     ]
    }
   ],
   "source": [
    "# === MobileGAN: A PyTorch Implementation for Generating Synthetic Attack Data ===\n",
    "# Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 64\n",
    "feature_dim = 78\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "epochs = 100\n",
    "\n",
    "# ===== Generator =====\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, feature_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# ===== Discriminator =====\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ===== Load and prepare attack data =====\n",
    "X_attack_tensor = torch.tensor(X_attack, dtype=torch.float32)\n",
    "train_loader = DataLoader(TensorDataset(X_attack_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ===== Initialize models and training utils =====\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# ===== Training Loop =====\n",
    "for epoch in range(epochs):\n",
    "    for real_batch, in train_loader:\n",
    "        real_batch = real_batch.to(device)\n",
    "        batch_size_curr = real_batch.size(0)\n",
    "\n",
    "        # Real and fake label smoothing\n",
    "        real_labels = torch.ones(batch_size_curr, 1).uniform_(0.9, 1.0).to(device)\n",
    "        fake_labels = torch.zeros(batch_size_curr, 1).uniform_(0.0, 0.1).to(device)\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        z = torch.randn(batch_size_curr, latent_dim).to(device)\n",
    "        fake_data = generator(z)\n",
    "\n",
    "        d_real = discriminator(real_batch)\n",
    "        d_fake = discriminator(fake_data.detach())\n",
    "\n",
    "        loss_real = criterion(d_real, real_labels)\n",
    "        loss_fake = criterion(d_fake, fake_labels)\n",
    "        d_loss = loss_real + loss_fake\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # === Train Generator ===\n",
    "        z = torch.randn(batch_size_curr, latent_dim).to(device)\n",
    "        fake_data = generator(z)\n",
    "        g_loss = criterion(discriminator(fake_data), real_labels)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "        optimizer_G.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# ===== Save the trained generator =====\n",
    "torch.save(generator.state_dict(), \"mobilegan_generator_stable.pth\")\n",
    "\n",
    "# ===== Function to generate synthetic samples =====\n",
    "def generate_synthetic_samples(generator, num_samples=1000):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        synthetic_data = generator(z)\n",
    "    return synthetic_data.cpu().numpy()\n",
    "\n",
    "# ===== Generate and Save Synthetic Attack Data =====\n",
    "synthetic_attacks = generate_synthetic_samples(generator, num_samples=1000)\n",
    "df_synthetic = pd.DataFrame(synthetic_attacks)\n",
    "df_synthetic.to_csv(\"synthetic_attacks.csv\", index=False)\n",
    "print(\" Synthetic attack data saved as 'synthetic_attacks.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c61f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 8.9143\n",
      "Epoch 2/10, Loss: 8.7138\n",
      "Epoch 3/10, Loss: 8.5955\n",
      "Epoch 4/10, Loss: 8.5333\n",
      "Epoch 5/10, Loss: 8.4487\n",
      "Epoch 6/10, Loss: 8.4785\n",
      "Epoch 7/10, Loss: 8.4330\n",
      "Epoch 8/10, Loss: 8.4304\n",
      "Epoch 9/10, Loss: 8.3908\n",
      "Epoch 10/10, Loss: 8.3658\n",
      "\n",
      "ðŸ“Š Discriminator Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6126    0.7750    0.6843       200\n",
      "         1.0     0.6939    0.5100    0.5879       200\n",
      "\n",
      "    accuracy                         0.6425       400\n",
      "   macro avg     0.6533    0.6425    0.6361       400\n",
      "weighted avg     0.6533    0.6425    0.6361       400\n",
      "\n",
      "ROC-AUC Score: 0.7461\n",
      "\n",
      "Confusion Matrix:\n",
      "[[155  45]\n",
      " [ 98 102]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "real_attacks = X_attack\n",
    "synthetic_attacks = pd.read_csv(\"synthetic_attacks.csv\").values\n",
    "\n",
    "# Balance real and synthetic samples\n",
    "min_len = min(len(real_attacks), len(synthetic_attacks))\n",
    "real_balanced = real_attacks[:min_len]\n",
    "synthetic_balanced = synthetic_attacks[:min_len]\n",
    "\n",
    "# Assign labels\n",
    "real_labels = np.ones((min_len, 1))\n",
    "synthetic_labels = np.zeros((min_len, 1))\n",
    "\n",
    "# Combine for evaluation\n",
    "X_eval = np.vstack([real_balanced, synthetic_balanced])\n",
    "y_eval = np.vstack([real_labels, synthetic_labels]).flatten()\n",
    "\n",
    "# === PCA + t-SNE Visualization ===\n",
    "def plot_visualizations(X_data, y_data, title_suffix=\"\"):\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_data)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_data, palette=[\"blue\", \"red\"], alpha=0.6)\n",
    "    plt.title(f\"PCA Visualization: Real (1) vs Synthetic (0) {title_suffix}\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.legend(title=\"Label\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_data)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y_data, palette=[\"blue\", \"red\"], alpha=0.6)\n",
    "    plt.title(f\"t-SNE Visualization: Real (1) vs Synthetic (0) {title_suffix}\")\n",
    "    plt.xlabel(\"t-SNE Dim 1\")\n",
    "    plt.ylabel(\"t-SNE Dim 2\")\n",
    "    plt.legend(title=\"Label\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_visualizations(X_eval, y_eval, \"(Attack Vectors)\")\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
    "    X_eval, y_eval, test_size=0.2, random_state=42, stratify=y_eval\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_eval, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_eval, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_eval, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_eval, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# DataLoader\n",
    "eval_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=78):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "disc_eval = Discriminator().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(disc_eval.parameters(), lr=0.0001)\n",
    "\n",
    "# Train discriminator\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    disc_eval.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in eval_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = disc_eval(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "disc_eval.eval()\n",
    "with torch.no_grad():\n",
    "    preds = disc_eval(X_test_tensor.to(device)).cpu().numpy().flatten()\n",
    "    preds_binary = (preds >= 0.5).astype(int)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nðŸ“Š Discriminator Evaluation Report:\")\n",
    "print(classification_report(y_test_eval, preds_binary, digits=4))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test_eval, preds):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_eval, preds_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23445e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
