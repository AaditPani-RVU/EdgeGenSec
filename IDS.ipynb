{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cf9c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training GAN on attack data...\n",
      "[Epoch 1/100] D Loss: 1.2886, G Loss: 0.9266\n",
      "[Epoch 2/100] D Loss: 1.2572, G Loss: 1.0055\n",
      "[Epoch 3/100] D Loss: 1.2176, G Loss: 0.9975\n",
      "[Epoch 4/100] D Loss: 1.3333, G Loss: 0.8921\n",
      "[Epoch 5/100] D Loss: 1.2326, G Loss: 1.1107\n",
      "[Epoch 6/100] D Loss: 1.1786, G Loss: 0.9331\n",
      "[Epoch 7/100] D Loss: 1.2217, G Loss: 0.9130\n",
      "[Epoch 8/100] D Loss: 1.2704, G Loss: 0.9149\n",
      "[Epoch 9/100] D Loss: 1.0895, G Loss: 1.0676\n",
      "[Epoch 10/100] D Loss: 1.1582, G Loss: 1.0705\n",
      "[Epoch 11/100] D Loss: 1.1698, G Loss: 1.0283\n",
      "[Epoch 12/100] D Loss: 1.1342, G Loss: 0.9903\n",
      "[Epoch 13/100] D Loss: 1.0597, G Loss: 1.1789\n",
      "[Epoch 14/100] D Loss: 1.0322, G Loss: 1.2528\n",
      "[Epoch 15/100] D Loss: 1.0615, G Loss: 1.1298\n",
      "[Epoch 16/100] D Loss: 1.0078, G Loss: 1.1588\n",
      "[Epoch 17/100] D Loss: 1.0040, G Loss: 1.3260\n",
      "[Epoch 18/100] D Loss: 1.0890, G Loss: 1.2601\n",
      "[Epoch 19/100] D Loss: 0.9194, G Loss: 1.3971\n",
      "[Epoch 20/100] D Loss: 0.9379, G Loss: 1.3283\n",
      "[Epoch 21/100] D Loss: 0.9752, G Loss: 1.2962\n",
      "[Epoch 22/100] D Loss: 0.9500, G Loss: 1.3073\n",
      "[Epoch 23/100] D Loss: 0.9102, G Loss: 1.3499\n",
      "[Epoch 24/100] D Loss: 0.9583, G Loss: 1.4164\n",
      "[Epoch 25/100] D Loss: 0.9620, G Loss: 1.3211\n",
      "[Epoch 26/100] D Loss: 0.9973, G Loss: 1.3118\n",
      "[Epoch 27/100] D Loss: 0.9404, G Loss: 1.3278\n",
      "[Epoch 28/100] D Loss: 0.9362, G Loss: 1.2873\n",
      "[Epoch 29/100] D Loss: 0.9142, G Loss: 1.3320\n",
      "[Epoch 30/100] D Loss: 0.9359, G Loss: 1.2665\n",
      "[Epoch 31/100] D Loss: 0.9502, G Loss: 1.3025\n",
      "[Epoch 32/100] D Loss: 0.9671, G Loss: 1.3224\n",
      "[Epoch 33/100] D Loss: 1.0067, G Loss: 1.2422\n",
      "[Epoch 34/100] D Loss: 0.9867, G Loss: 1.1820\n",
      "[Epoch 35/100] D Loss: 1.0083, G Loss: 1.2959\n",
      "[Epoch 36/100] D Loss: 0.9664, G Loss: 1.5586\n",
      "[Epoch 37/100] D Loss: 0.9525, G Loss: 1.3817\n",
      "[Epoch 38/100] D Loss: 0.9071, G Loss: 1.3233\n",
      "[Epoch 39/100] D Loss: 1.3206, G Loss: 1.3686\n",
      "[Epoch 40/100] D Loss: 0.9377, G Loss: 1.2697\n",
      "[Epoch 41/100] D Loss: 0.9832, G Loss: 1.3276\n",
      "[Epoch 42/100] D Loss: 0.9854, G Loss: 1.2051\n",
      "[Epoch 43/100] D Loss: 0.9357, G Loss: 1.3074\n",
      "[Epoch 44/100] D Loss: 1.0389, G Loss: 1.2988\n",
      "[Epoch 45/100] D Loss: 0.9506, G Loss: 1.2053\n",
      "[Epoch 46/100] D Loss: 0.9605, G Loss: 1.3342\n",
      "[Epoch 47/100] D Loss: 0.8552, G Loss: 1.3455\n",
      "[Epoch 48/100] D Loss: 1.0179, G Loss: 1.4135\n",
      "[Epoch 49/100] D Loss: 0.8930, G Loss: 1.6022\n",
      "[Epoch 50/100] D Loss: 0.9843, G Loss: 1.2902\n",
      "[Epoch 51/100] D Loss: 0.9770, G Loss: 1.3706\n",
      "[Epoch 52/100] D Loss: 0.9731, G Loss: 1.2960\n",
      "[Epoch 53/100] D Loss: 0.9708, G Loss: 1.3642\n",
      "[Epoch 54/100] D Loss: 0.9025, G Loss: 1.4111\n",
      "[Epoch 55/100] D Loss: 1.0135, G Loss: 1.5880\n",
      "[Epoch 56/100] D Loss: 0.9012, G Loss: 1.4604\n",
      "[Epoch 57/100] D Loss: 0.9632, G Loss: 1.3027\n",
      "[Epoch 58/100] D Loss: 0.9806, G Loss: 1.3514\n",
      "[Epoch 59/100] D Loss: 1.0024, G Loss: 1.3398\n",
      "[Epoch 60/100] D Loss: 0.8838, G Loss: 1.3379\n",
      "[Epoch 61/100] D Loss: 1.0468, G Loss: 1.3838\n",
      "[Epoch 62/100] D Loss: 0.9913, G Loss: 1.3500\n",
      "[Epoch 63/100] D Loss: 0.8988, G Loss: 1.3314\n",
      "[Epoch 64/100] D Loss: 0.8679, G Loss: 1.1430\n",
      "[Epoch 65/100] D Loss: 1.0251, G Loss: 1.2740\n",
      "[Epoch 66/100] D Loss: 1.0468, G Loss: 1.2922\n",
      "[Epoch 67/100] D Loss: 1.0047, G Loss: 1.2636\n",
      "[Epoch 68/100] D Loss: 0.9465, G Loss: 1.2170\n",
      "[Epoch 69/100] D Loss: 0.9869, G Loss: 1.3880\n",
      "[Epoch 70/100] D Loss: 0.9870, G Loss: 1.2033\n",
      "[Epoch 71/100] D Loss: 1.0358, G Loss: 1.1915\n",
      "[Epoch 72/100] D Loss: 1.0096, G Loss: 1.3697\n",
      "[Epoch 73/100] D Loss: 0.8918, G Loss: 1.3660\n",
      "[Epoch 74/100] D Loss: 0.9462, G Loss: 1.2597\n",
      "[Epoch 75/100] D Loss: 0.9878, G Loss: 1.2328\n",
      "[Epoch 76/100] D Loss: 1.0148, G Loss: 1.1773\n",
      "[Epoch 77/100] D Loss: 1.0603, G Loss: 1.2138\n",
      "[Epoch 78/100] D Loss: 0.9479, G Loss: 1.4789\n",
      "[Epoch 79/100] D Loss: 0.9701, G Loss: 1.1813\n",
      "[Epoch 80/100] D Loss: 0.9112, G Loss: 1.2029\n",
      "[Epoch 81/100] D Loss: 0.9549, G Loss: 1.2177\n",
      "[Epoch 82/100] D Loss: 1.0482, G Loss: 1.1711\n",
      "[Epoch 83/100] D Loss: 0.9558, G Loss: 1.2427\n",
      "[Epoch 84/100] D Loss: 0.9728, G Loss: 1.1851\n",
      "[Epoch 85/100] D Loss: 0.9715, G Loss: 1.3038\n",
      "[Epoch 86/100] D Loss: 1.0192, G Loss: 1.3820\n",
      "[Epoch 87/100] D Loss: 1.0205, G Loss: 1.1827\n",
      "[Epoch 88/100] D Loss: 1.0176, G Loss: 1.4369\n",
      "[Epoch 89/100] D Loss: 1.0337, G Loss: 1.2656\n",
      "[Epoch 90/100] D Loss: 1.0625, G Loss: 1.1825\n",
      "[Epoch 91/100] D Loss: 0.9163, G Loss: 1.2112\n",
      "[Epoch 92/100] D Loss: 1.1087, G Loss: 1.2317\n",
      "[Epoch 93/100] D Loss: 1.0348, G Loss: 1.1966\n",
      "[Epoch 94/100] D Loss: 1.0591, G Loss: 1.1682\n",
      "[Epoch 95/100] D Loss: 1.0171, G Loss: 1.2399\n",
      "[Epoch 96/100] D Loss: 1.0473, G Loss: 1.2280\n",
      "[Epoch 97/100] D Loss: 1.1521, G Loss: 1.1531\n",
      "[Epoch 98/100] D Loss: 1.0320, G Loss: 1.1935\n",
      "[Epoch 99/100] D Loss: 0.9778, G Loss: 1.3257\n",
      "[Epoch 100/100] D Loss: 0.9812, G Loss: 1.1411\n",
      "âœ… GAN Generator saved to mobilegan_generator_stable.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# === Config ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 64\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "gan_epochs = 100\n",
    "ids_epochs = 10\n",
    "feature_file = \"features_used.json\"\n",
    "gan_model_path = \"mobilegan_generator_stable.pth\"\n",
    "synthetic_csv = \"synthetic_attacks.csv\"\n",
    "torchscript_path = \"ids_cnn_edgegensec.pt\"\n",
    "\n",
    "# === Load and clean dataset ===\n",
    "df = pd.read_csv(r\"CICIDS2017_Multiclass_Balanced_5k.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'].astype(str))\n",
    "benign_label = label_encoder.transform([\"BENIGN\"])[0]\n",
    "\n",
    "# Drop non-numeric columns except Label\n",
    "non_numerics = df.select_dtypes(include=['object']).columns\n",
    "df = df.drop(non_numerics.difference(['Label']), axis=1)\n",
    "\n",
    "# === Feature and label split ===\n",
    "X = df.drop('Label', axis=1).values\n",
    "y = df['Label'].values\n",
    "features = df.drop('Label', axis=1).columns.tolist()\n",
    "\n",
    "# Save features\n",
    "with open(feature_file, \"w\") as f:\n",
    "    json.dump(features, f)\n",
    "\n",
    "# === Normalize and split ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_attack = X_train[y_train != benign_label]  # Only attack flows\n",
    "\n",
    "# === MobileGAN Models ===\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, feature_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# === Train GAN ===\n",
    "feature_dim = X_attack.shape[1]\n",
    "X_attack_tensor = torch.tensor(X_attack, dtype=torch.float32)\n",
    "train_loader = DataLoader(TensorDataset(X_attack_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "generator = Generator(feature_dim).to(device)\n",
    "discriminator = Discriminator(feature_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "print(\"ðŸš€ Training GAN on attack data...\")\n",
    "for epoch in range(gan_epochs):\n",
    "    for real_batch, in train_loader:\n",
    "        real_batch = real_batch.to(device)\n",
    "        b = real_batch.size(0)\n",
    "\n",
    "        real_labels = torch.ones(b, 1).uniform_(0.9, 1.0).to(device)\n",
    "        fake_labels = torch.zeros(b, 1).uniform_(0.0, 0.1).to(device)\n",
    "\n",
    "        z = torch.randn(b, latent_dim).to(device)\n",
    "        fake_data = generator(z)\n",
    "\n",
    "        d_loss = criterion(discriminator(real_batch), real_labels) + \\\n",
    "                 criterion(discriminator(fake_data.detach()), fake_labels)\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        z = torch.randn(b, latent_dim).to(device)\n",
    "        g_loss = criterion(discriminator(generator(z)), real_labels)\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{gan_epochs}] D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "torch.save(generator.state_dict(), gan_model_path)\n",
    "print(f\"âœ… GAN Generator saved to {gan_model_path}\")\n",
    "\n",
    "# === Generate synthetic attacks ===\n",
    "def generate_synthetic_samples(generator, n=1000):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n, latent_dim).to(device)\n",
    "        return generator(z).cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85808052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Synthetic attacks saved to synthetic_attacks.csv\n"
     ]
    }
   ],
   "source": [
    "synthetic_attacks = generate_synthetic_samples(generator, n=1000)\n",
    "pd.DataFrame(synthetic_attacks).to_csv(synthetic_csv, index=False)\n",
    "print(f\"âœ… Synthetic attacks saved to {synthetic_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2da9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 63.0885\n",
      "Epoch 2/15, Loss: 35.4427\n",
      "Epoch 3/15, Loss: 29.6421\n",
      "Epoch 4/15, Loss: 24.0962\n",
      "Epoch 5/15, Loss: 21.2017\n",
      "Epoch 6/15, Loss: 19.7634\n",
      "Epoch 7/15, Loss: 18.9510\n",
      "Epoch 8/15, Loss: 18.6097\n",
      "Epoch 9/15, Loss: 17.4554\n",
      "Epoch 10/15, Loss: 17.4702\n",
      "Epoch 11/15, Loss: 16.9774\n",
      "Epoch 12/15, Loss: 16.5780\n",
      "Epoch 13/15, Loss: 15.7725\n",
      "Epoch 14/15, Loss: 15.5160\n",
      "Epoch 15/15, Loss: 14.8289\n",
      "\n",
      "ðŸ“Š IDS Classifier Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9833    0.8850    0.9316       800\n",
      "         1.0     0.9869    0.9983    0.9926      6957\n",
      "\n",
      "    accuracy                         0.9866      7757\n",
      "   macro avg     0.9851    0.9416    0.9621      7757\n",
      "weighted avg     0.9866    0.9866    0.9863      7757\n",
      "\n",
      "ROC-AUC Score: 0.9927\n",
      "Confusion Matrix:\n",
      "[[ 708   92]\n",
      " [  12 6945]]\n",
      "âœ… TorchScript model saved as 'ids_cnn_edgegensec.pt'\n"
     ]
    }
   ],
   "source": [
    "# === ONLY NEW IMPORTS ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# === Load and Preprocess CSV ===\n",
    "df = pd.read_csv(\"CICIDS2017_Multiclass_Balanced_5k.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# === Label Encoding ===\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "df['Label'] = label_encoder.transform(df['Label'].astype(str))\n",
    "\n",
    "# === Drop non-numeric columns ===\n",
    "non_numerics = df.select_dtypes(include=['object']).columns\n",
    "df = df.drop(non_numerics.difference(['Label']), axis=1)\n",
    "\n",
    "# === Synthetic Data Generation Using Gaussian Noise ===\n",
    "def generate_synthetic_data(real_df, label, target_size):\n",
    "    real_samples = real_df[real_df['Label'] == label].drop(\"Label\", axis=1).values\n",
    "    if len(real_samples) == 0:\n",
    "        raise ValueError(\"No real samples found for the label {}\".format(label))\n",
    "    \n",
    "    mu = real_samples.mean(axis=0)\n",
    "    sigma = real_samples.std(axis=0)\n",
    "    synthetic_data = np.random.normal(loc=mu, scale=sigma, size=(target_size, real_samples.shape[1]))\n",
    "    df_synth = pd.DataFrame(synthetic_data, columns=real_df.columns[:-1])\n",
    "    df_synth['Label'] = label_encoder.transform([\"SYNTHETIC\"])[0] if \"SYNTHETIC\" in label_encoder.classes_ else df['Label'].max() + 1\n",
    "    return df_synth\n",
    "\n",
    "# === Generate and Combine Synthetic Data ===\n",
    "synth_df = generate_synthetic_data(df, label=df['Label'].mode()[0], target_size=5000)\n",
    "df_combined = pd.concat([df, synth_df], ignore_index=True)\n",
    "\n",
    "# === Feature/Label separation and standardization ===\n",
    "feature_cols = df_combined.drop(\"Label\", axis=1).columns.tolist()\n",
    "X = df_combined[feature_cols].values\n",
    "y = df_combined[\"Label\"].values\n",
    "input_len = len(feature_cols)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "with open(\"features_list.pkl\", \"wb\") as f:\n",
    "    joblib.dump(feature_cols, f)\n",
    "\n",
    "# === Train/Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# === Torch Dataloaders ===\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=128)\n",
    "\n",
    "# === Multiclass CNN Architecture ===\n",
    "class CNNMulticlassIDS(nn.Module):\n",
    "    def __init__(self, input_len, num_classes):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_len // 4) * 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# === Training ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(np.unique(y))\n",
    "model = CNNMulticlassIDS(input_len=input_len, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# === Evaluation ===\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "print(\"\\n\\U0001F4CA Multiclass IDS Evaluation:\")\n",
    "print(classification_report(y_test, all_preds))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, all_preds))\n",
    "\n",
    "# === Save TorchScript model ===\n",
    "example_input = torch.rand(1, 1, input_len).to(device)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "traced_model.save(\"ids_cnn_multiclass.pt\")\n",
    "print(\"âœ… Saved: ids_cnn_multiclass.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7679502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
